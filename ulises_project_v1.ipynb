{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las librerías necesarias\n",
    "# Load the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import time\n",
    "import json\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "import openai\n",
    "import spacy\n",
    "from docx import Document\n",
    "\n",
    "from anthropic import Anthropic\n",
    "anthropic = Anthropic(api_key='')  # Replace with your Anthropic API key\n",
    "openai.api_key = ''  # Replace with your OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debes introducir la ruta de la carpeta donde los archivos intermedios que permiten verificar el razonamiento del sistema van a ser almacenados\n",
    "# You must enter the path of the folder where the intermediate files that allow verifying the system's reasoning will be stored\n",
    "\n",
    "folder_path = r'your_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Introduce los hechos del caso: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de Limpieza y Procesamiento / Cleaning and Processing Functions\n",
    "\n",
    "En esta sección se presentan las funciones básicas que utiliza nuestra Intelgencia Artificial para el procesamiento de la data textual\n",
    "\n",
    "---\n",
    "\n",
    "In this section, the basic functions used by our Artificial Intelligence for processing textual data are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para limpieza y procesamiento de texto\n",
    "# For text cleaning and processing\n",
    "\n",
    "\n",
    "def clean_text_column(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    df[column_name] = df[column_name].str.replace(r'[\\n\\r\\t]', ' ', regex=True).str.strip()\n",
    "    df[column_name] = df[column_name].str.replace(r'\\s+', ' ', regex=True)\n",
    "    df[column_name] = df[column_name].str.replace(r'\\xa0', ' ', regex=True)\n",
    "    df[column_name] = df[column_name].str.replace('*', '', regex=False)\n",
    "    df[column_name] = df[column_name].astype(str)\n",
    "    df[column_name] = df[column_name].str.replace(r'[\\uF02D\\uF02E]', '', regex=True) \n",
    "    df[column_name] = df[column_name].str.lower()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para generar resúmenes de texto\n",
    "# For generating text summaries\n",
    "\n",
    "def generate_summaries(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Elabora un resumen detallado en formato de párrafos del siguiente extracto, indicando en la última oración las páginas resumidas\"},\n",
    "            \n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=16383,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para la lectura y procesamiento de json schemas\n",
    "# For reading and processing JSON schemas\n",
    "\n",
    "SCHEMA_DIR = r'your_schemas_path'     \n",
    "\n",
    "def load_json_schema(schema_name):\n",
    "    schema_path = os.path.join(SCHEMA_DIR, schema_name)\n",
    "    with open(schema_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "schema = load_json_schema(\"schema.json\")\n",
    "\n",
    "def extract_json_from_response(response_content):\n",
    "    match = re.search(r\"\\{.*\\}\", response_content, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "def convert_to_string(data):\n",
    "    for key in data:\n",
    "        if isinstance(data[key], list):\n",
    "            data[key] = [str(item) if item is not None else \"N/A\" for item in data[key]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'es_core_news_sm'...\n",
      "Model installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download the model if not installed\n",
    "# Descarga el modelo si no ha sido instalado\n",
    "\n",
    "try:\n",
    "    spacy.load(\"es_core_news_sm\")\n",
    "    print(\"Model 'es_core_news_sm' is already installed.\")\n",
    "except OSError:\n",
    "    # Download the model if not installed\n",
    "    print(\"Downloading 'es_core_news_sm'...\")\n",
    "    os.system(\"python -m spacy download es_core_news_sm\")\n",
    "    print(\"Model installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para la generación de embeddings \n",
    "# For generating embeddings\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    max_tokens = 8191  \n",
    "    rate_limit_wait_time = 60  \n",
    "    timeout_wait_time = 20 \n",
    "\n",
    "    def split_texts(texts, max_tokens):\n",
    "        \"\"\"Split texts into chunks based on the maximum token limit.\"\"\"\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        for text in texts:\n",
    "            text_length = len(text.split())  \n",
    "            if current_length + text_length > max_tokens:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = [text]\n",
    "                current_length = text_length\n",
    "            else:\n",
    "                current_chunk.append(text)\n",
    "                current_length += text_length\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        return chunks\n",
    "\n",
    "    def get_embeddings_for_chunk(chunk):\n",
    "        \"\"\"Get embeddings for a single chunk of texts.\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                if not all(isinstance(c, str) for c in chunk):\n",
    "                    raise ValueError(\"Chunk contains non-string elements.\")\n",
    "                \n",
    "                response = openai.Embedding.create(\n",
    "                    model=\"text-embedding-3-small\", \n",
    "                    input=chunk \n",
    "                )\n",
    "                return [data['embedding'] for data in response['data']]\n",
    "            except openai.error.InvalidRequestError as e:\n",
    "                print(f\"Invalid request: {e}. Chunk: {chunk}\")\n",
    "                raise\n",
    "            except openai.error.RateLimitError:\n",
    "                print(f\"Rate limit exceeded. Waiting for {rate_limit_wait_time} seconds...\")\n",
    "                time.sleep(rate_limit_wait_time)\n",
    "            except openai.error.APIConnectionError:\n",
    "                print(f\"Connection timed out. Waiting for {timeout_wait_time} seconds...\")\n",
    "                time.sleep(timeout_wait_time)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                raise\n",
    "\n",
    "    chunks = split_texts(texts, max_tokens)\n",
    "    embeddings = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            embeddings.extend(get_embeddings_for_chunk(chunk))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i}: {chunk}\") \n",
    "            continue\n",
    "\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando la base de datos que contiene las decisiones en materia de Libre Competencia / Preparing the database containing decisions on Competition Law\n",
    "\n",
    "Se cargan las bases de datos que servirán como fuente de conocimiento externo para nuestro sistema.\n",
    "\n",
    "---\n",
    "\n",
    "The databases that will serve as external knowledge sources for our system are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos y procesamos las bases de datos necesarias\n",
    "# Load and process the necessary databases\n",
    "\n",
    "jurisprudencia = pd.read_pickle(r\"cl_corpus_2009_2024.pkl\")\n",
    "metadata = pd.read_pickle(r\"metadata.pkl\") \n",
    "jurisprudencia = pd.merge(jurisprudencia, metadata, on=['ID', 'Subfolder'])\n",
    "url_df = pd.read_csv(r\"pdf_urls.csv\")\n",
    "url_df['ID'] = url_df['ID'].str.replace('.pdf', '')\n",
    "jurisprudencia = pd.merge(jurisprudencia, url_df, how='left', on='ID')\n",
    "replace_dict = {\n",
    "    'ccp_2009_2024_ant': 'Cuerpo Colegiado Permanente del OSIPTEL',\n",
    "    'tsc_2009_2024_ant': 'Tribunal de Solución de Controversias del OSIPTEL',\n",
    "    'sdc_2009_2024_ant': 'Sala de Defensa de la Competencia del INDECOPI',\n",
    "    'clc_2009_2024_ant': 'Comisión de Defensa de la Libre Competencia del INDECOPI'\n",
    "}\n",
    "\n",
    "jurisprudencia['Subfolder'] = jurisprudencia['Subfolder'].replace(replace_dict)\n",
    "\n",
    "dl_1034 = pd.read_excel(r\"dl_1034.xlsx\") # Contiene los artículos del TUO de la LRCA, así como los objetivos a interpretar por artículo\n",
    "\n",
    "jurisprudencia = clean_text_column(jurisprudencia, 'Text')\n",
    "dl_1034 = clean_text_column(dl_1034, 'rule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programas\\anaconda3\\envs\\ulises\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\Programas\\anaconda3\\envs\\ulises\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Complementamos la carga de la base de datos con los embeddings\n",
    "# Complete the database loading with the embeddings\n",
    "\n",
    "embeddings_path = r\"embeddings.pkl\"\n",
    "\n",
    "with open(embeddings_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    summary_embeddings = data['summary_embeddings']\n",
    "    saved_metadata = data['metadata']\n",
    "    saved_summaries = data['summaries']\n",
    "\n",
    "df_to_merge = jurisprudencia[['Resolución', 'Subfolder', 'Fecha', 'Expediente', 'ID', 'url']].drop_duplicates(subset='ID')\n",
    "saved_metadata = pd.merge(saved_metadata, df_to_merge, on='ID', how='left')\n",
    "\n",
    "with open(r\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "with open(r\"tfidf_matrix_cl_corpus.pkl\", \"rb\") as f:\n",
    "    tfidf_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Módulo Issue / Issue Module\n",
    "\n",
    "El objetivo principal de este módulo es formular el **Legal Judgment Prediction (LJP)** propuesto, en su forma clásica. Este enfoque se centra en determinar si los hechos presentados en un caso constituyen o no una violación a la ley. \n",
    "\n",
    "En este contexto particular, el **issue** planteado busca responder si los hechos específicos del caso infringen los requisitos establecidos en la **ley de competencia** para sancionar una práctica colusoria horizontal.\n",
    "\n",
    "En este módulo, utilizamos exclusivamente el siguiente código para plantear el problema legal a resolver.\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "In this module, we exclusively use the following code to frame the legal question to be addressed:\n",
    "\n",
    "\n",
    "The primary goal of this module is to implement the proposed **Legal Judgment Prediction (LJP)** in its classical form. This approach focuses on determining whether the facts presented in a case constitute a violation of the law. \n",
    "\n",
    "In this specific context, the **issue** aims to determine whether the case's specific facts infringe the requirements set by **competition law** to sanction a horizontal collusive practice.\n",
    "\n",
    "In this module, we exclusively use the following code to frame the legal question to be addressed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue = f\"¿Los siguientes hechos '{query}' SI cumple O NO cumple con los requisitos que establece la ley de competencia para sancionar una conducta?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Módulo Regla / Rule Module\n",
    "\n",
    "Este módulo consta de una secuencia de pasos diseñados para facilitar la aplicación e interpretación de los artículos legales relevantes para resolver prácticas colusorias horizontales.\n",
    "\n",
    "1. **Generación de Preguntas Específicas**  \n",
    "   Se utiliza una función para generar preguntas precisas sobre la aplicación de los artículos relevantes al caso. Esta función toma como datos de entrada las columnas **goal**, **rule** y los **hechos del caso**, para formular las preguntas más adecuadas que permitan interpretar el artículo y su aplicación a los hechos.\n",
    "\n",
    "2. **Recuperación de Decisiones Relevantes**  \n",
    "   Las preguntas generadas por un modelo de lenguaje son utilizadas para recuperar decisiones del INDECOPI u OSIPTEL. Este proceso de recuperación emplea dos algoritmos:  \n",
    "   - **Embedding Semántico**: Utiliza el modelo `text-embedding-3-small` para capturar relaciones semánticas entre la Pregunta Específica y los extractos de las decisiones.\n",
    "   - **Análisis TF-IDF**: Complementa la búsqueda con un enfoque estadístico basado en frecuencia de términos.\n",
    "\n",
    "   Ambos algoritmos identifican los extractos más relevantes que puedan contener las respuestas a las preguntas. Estos extractos se combinan y se recupera la página que contiene el extracto, junto con dos páginas consecutivas para garantizar que se aborde la mayor cantidad de contexto posible.\n",
    "\n",
    "3. **Resumir la Información Extraída**  \n",
    "   El contenido extraído se resume utilizando otro modelo de lenguaje, condensando las páginas recuperadas en información relevante y concisa.\n",
    "\n",
    "4. **Responder las Preguntas Específicas**  \n",
    "   Finalmente, un modelo de lenguaje utiliza los resúmenes generados para responder a las preguntas específicas sobre la aplicación de los artículos relevantes para evaluar la legalidad de una práctica colusoria horizontal.\n",
    "\n",
    "Este proceso asegura un análisis robusto, integrando la recuperación de información generada por las agencias de competencia, la síntesis de información y respuestas precisas a preguntas específicas en Derecho de la Libre Competencia.\n",
    "\n",
    "---\n",
    "\n",
    "This module consists of a sequence of steps designed to facilitate the application and interpretation of relevant legal provisions for resolving horizontal collusive practices.\n",
    "\n",
    "1. **Generating Specific Questions**  \n",
    "   A function is used to generate precise questions about the application of relevant articles to the case. This function takes input from the **goal**, **rule**, and **facts of the case** columns to formulate the most appropriate questions for interpreting the article and its application to the facts.\n",
    "\n",
    "2. **Retrieving Relevant Decisions**  \n",
    "   The questions generated by a language model are used to retrieve decisions from **INDECOPI** or **OSIPTEL**. This retrieval process employs two algorithms:  \n",
    "   - **Semantic Embedding**: Utilizes the `text-embedding-3-small` model to capture semantic relationships between the Specific Question and decision excerpts.  \n",
    "   - **TF-IDF Analysis**: Complements the search with a statistical term-frequency approach.  \n",
    "\n",
    "   Both algorithms identify the most relevant excerpts that may contain answers to the questions. These excerpts are combined, retrieving the page containing the excerpt along with two consecutive pages to ensure adequate context is captured.\n",
    "\n",
    "3. **Summarizing Extracted Information**  \n",
    "   The extracted content is summarized using another language model, condensing the retrieved pages into concise and relevant information.\n",
    "\n",
    "4. **Answering Specific Questions**  \n",
    "   Finally, a language model uses the generated summaries to answer the specific questions regarding the application of the relevant articles to assess the legality of a horizontal collusive practice.\n",
    "\n",
    "This process ensures a robust analysis, integrating the retrieval of information generated by competition agencies, the synthesis of information, and precise answers to specific questions in Competition Law.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a elaborar la data de entrada que servirá para obtener nuestras preguntas sobre la correcta aplicación del artículo de la LRCA\n",
    "# Es importante que las preguntas, aunque abstractas, sean capaces de mantener cierta orientación hacia los Hechos del Caso. \n",
    "# Es por tal razón que se incluye el Objetivo Planteado y los Hechos del Caso, así como el artículo que debe ser interpretado.\n",
    "\n",
    "# We will create the input data that will serve to generate our questions about the correct application of the LRCA article\n",
    "# It is important that the questions, although abstract, are able to maintain some orientation towards the Case Facts.\n",
    "# For this reason, the Stated Objective and the Case Facts are included, as well as the article that must be interpreted.\n",
    "\n",
    "\n",
    "dl_1034['input'] = dl_1034.apply(lambda row: f'Objetivo Planteado:{row[\"goal\"]} Hechos del Caso {query} Artículo a Interpretar: {row[\"rule\"]}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def issue_per_rule(text, max_tokens=200, temperature=0, top_p=1):\n",
    "    \"\"\"\n",
    "    Genera preguntas abstractas basadas en un texto de caso usando la API de Claude de Anthropic.\n",
    "    \n",
    "    La función toma un texto que contiene el artículo que debe ser aplicado a los hechos de un caso, \n",
    "    así como directivas para mejorar las preguntas mediante objetivos, y lo procesa a través \n",
    "    del modelo de lenguaje Claude y genera dos preguntas abstractas que ayudan a analizar el caso. \n",
    "    Las preguntas se generan de forma genérica, evitando nombres específicos y usando el término \n",
    "    \"empresa\" para referirse a las entidades involucradas. Es especialmente útil para análisis \n",
    "    de casos legales o empresariales donde se necesita abstraer los detalles específicos para \n",
    "    enfocarse en los principios o problemas fundamentales del caso.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        message = anthropic.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            system=\n",
    "                \"Elabora dos preguntas abstractas que sirvan para cumplir con el Objetivo Planteado a partir de los Hechos del Caso. No menciones nombres, solo refierete a ellos como empresa \"\n",
    "                \"Responde en el siguiente formato:\"\n",
    "                \"¿(Pregunta Abstracta 1)?\"\n",
    "                \"¿(Pregunta Abstracta 2)?\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }]\n",
    "        )\n",
    "        return message.content[0].text if isinstance(message.content, list) else message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "dl_1034['issue_per_rule'] = dl_1034['input'].apply(issue_per_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación mediante embeddings semánticos\n",
    "# Retrieval using semantic embeddings\n",
    "\n",
    "\n",
    "embeddings_rows = [] \n",
    "\n",
    "# Iterar sobre cada fila del dataFrame 'dl_1034'\n",
    "# Iterate over each row of the 'dl_1034' DataFrame\n",
    "\n",
    "for i, row in dl_1034.iterrows():\n",
    "    input_text = row['issue_per_rule']  \n",
    "    \n",
    "    query_embedding = get_embeddings([input_text])[0]\n",
    "    \n",
    "    cos_similarities = cosine_similarity([query_embedding], summary_embeddings)[0]\n",
    "    \n",
    "    # Obtener los índices de las 4 filas más similares (ordenadas de mayor a menor)\n",
    "    # Get the indices of the 4 most similar rows (sorted from highest to lowest)\n",
    "\n",
    "    top_indices = cos_similarities.argsort()[-4:][::-1]\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        # Acceder a los metadatos de la fila similar\n",
    "        # Access the metadata of the similar row\n",
    "\n",
    "        similar_row = saved_metadata.iloc[idx] \n",
    "        \n",
    "        # Agregar a la lista información de la fila original, la similitud y los metadatos de la fila similar\n",
    "        # Add to the list information from the original row, the similarity, and the metadata of the similar row\n",
    "\n",
    "        embeddings_rows.append({\n",
    "            \"section\": row['section'],               \n",
    "            \"rule\": row['rule'],                    \n",
    "            \"questions\": row['issue_per_rule'],      \n",
    "            \"Similarity\": cos_similarities[idx],    \n",
    "            \"ID\": similar_row['ID'],                 \n",
    "            \"age\": similar_row['Page'],            \n",
    "            \"authority\": similar_row['Subfolder'],   \n",
    "            \"decision\": similar_row['Resolución'], \n",
    "            \"case_file\": similar_row['Expediente'], \n",
    "            \"date\": similar_row['Fecha'],           \n",
    "            \"link\": similar_row['url'],            \n",
    "        })\n",
    "\n",
    "embeddings_df = pd.DataFrame(embeddings_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperación mediante TF-IDF\n",
    "# Retrieval using TF-IDF\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Función para preprocesar texto: lematización y eliminación de stopwords y puntuación\n",
    "# Function to preprocess text: lemmatization and removal of stopwords and punctuation\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Aplicar preprocesamiento a la columna 'issue_per_rule'\n",
    "# Apply preprocessing to the 'issue_per_rule' column\n",
    "\n",
    "dl_1034['Processed_Text'] = dl_1034['issue_per_rule'].apply(preprocess_text)\n",
    "\n",
    "ti_rows = []  \n",
    "\n",
    "# Iterar sobre cada fila del DataFrame 'dl_1034'\n",
    "# Iterate over each row of the 'dl_1034' DataFrame\n",
    "\n",
    "for i, row in dl_1034.iterrows():\n",
    "    query_text = row['Processed_Text']  \n",
    "    \n",
    "    # Transformar el texto de consulta al mismo espacio vectorial que 'tfidf_matrix'\n",
    "    # Transform the query text into the same vector space as 'tfidf_matrix'\n",
    "\n",
    "    query_vector = vectorizer.transform([query_text])\n",
    "    \n",
    "    # Calcular la similitud coseno entre el vector de consulta y toda la matriz tfidf\n",
    "    # Calculate the cosine similarity between the query vector and the entire tfidf matrix\n",
    "\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Obtener los índices de las 4 filas más similares (ordenadas de mayor a menor similitud)\n",
    "    # Get the indices of the 4 most similar rows (sorted from highest to lowest similarity)\n",
    "\n",
    "    top_indices = similarities.argsort()[-4:][::-1]\n",
    "    \n",
    "    # Para cada fila similar, almacenar la información relevante\n",
    "    # For each similar row, store the relevant information\n",
    "\n",
    "    for idx in top_indices:\n",
    "        # Acceder a los metadatos de la fila similar\n",
    "        # Access the metadata of the similar row\n",
    "\n",
    "        similar_row = jurisprudencia.iloc[idx]  \n",
    "        \n",
    "        # Agregar a la lista la información de la fila original y la similar\n",
    "        # Add the original row information and the similar row information to the list\n",
    "        \n",
    "        ti_rows.append({\n",
    "            \"section\": row['section'],               \n",
    "            \"rule\": row['rule'],                     \n",
    "            \"questions\": row['issue_per_rule'],     \n",
    "            \"Similarity\": similarities[idx],      \n",
    "            \"ID\": similar_row['ID'],                 \n",
    "            \"Page\": similar_row['Page'],            \n",
    "            \"authority\": similar_row['Subfolder'],   \n",
    "            \"decision\": similar_row['Resolución'],   \n",
    "            \"case_file\": similar_row['Expediente'],  \n",
    "            \"date\": similar_row['Fecha'],            \n",
    "            \"link\": similar_row['url'],              \n",
    "        })\n",
    "\n",
    "ti_df = pd.DataFrame(ti_rows)\n",
    "ti_df = ti_df.drop_duplicates(subset=['ID', 'Page'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar los DataFrames 'embeddings_df' y 'ti_df' en uno solo\n",
    "# Combine the 'embeddings_df' and 'ti_df' DataFrames into one\n",
    "\n",
    "retrieved_data = pd.concat([embeddings_df, ti_df], ignore_index=True)\n",
    "\n",
    "# Crear una lista para almacenar las filas adicionales\n",
    "# Create a list to store the additional rows\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "# Iterar a través de cada fila en 'retrieved_data'\n",
    "# Iterate through each row in 'retrieved_data'\n",
    "\n",
    "for _, row in retrieved_data.iterrows():\n",
    "    # Get the current ID and page of the row\n",
    "    # Obtener el ID y la página actual de la fila\n",
    "    current_id = row['ID']\n",
    "    current_page = row['Page']\n",
    "    \n",
    "    # Filtrar el DataFrame 'jurisprudencia' por filas con el mismo ID\n",
    "    # Filter the 'jurisprudencia' DataFrame for rows with the same ID\n",
    "\n",
    "    df_filtered = jurisprudencia[jurisprudencia['ID'] == current_id]\n",
    "    \n",
    "    # Ordenar las filas filtradas por la columna 'Page'\n",
    "    # Sort the filtered rows by the 'Page' column\n",
    "\n",
    "    df_filtered = df_filtered.sort_values(by='Page')\n",
    "    \n",
    "    # Encontrar explícitamente la página actual y las siguientes dos páginas\n",
    "    # Explicitly find the current page and the next two pages\n",
    "\n",
    "    additional_rows = df_filtered[df_filtered['Page'] >= current_page].head(3)\n",
    "    \n",
    "    # Agregar las filas adicionales a la lista 'new_rows'\n",
    "    # Add the additional rows to the 'new_rows' list\n",
    "\n",
    "    for _, additional_row in additional_rows.iterrows():\n",
    "        new_rows.append({\n",
    "            # Incluir todas las columnas de 'retrieved_data'\n",
    "            # Include all columns from 'retrieved_data'\n",
    "            **row.to_dict(), \n",
    "\n",
    "            # Página adicional de 'jurisprudencia' \n",
    "            # Additional page from 'jurisprudencia'\n",
    "            'pages': additional_row['Page'],\n",
    "\n",
    "            # Texto adicional, si existe en 'jurisprudencia'   \n",
    "            # Additional text, if it exists in 'jurisprudencia'\n",
    "            'Text': additional_row.get('Text', None),  \n",
    "\n",
    "        })\n",
    "\n",
    "rule = pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificar la columna 'Text' para incluir el formato solicitado\n",
    "# Modify the 'Text' column to include the requested format\n",
    "\n",
    "rule['Text'] = rule.apply(lambda row: f'PÁGINA({row[\"pages\"]}) {row[\"Text\"]} PÁGINA({row[\"pages\"]})', axis=1)\n",
    "\n",
    "rule = (\n",
    "    rule\n",
    "    .groupby(['section','questions','rule', 'decision', 'link', 'authority', 'date', 'case_file'], as_index=False)\n",
    "    .agg({\n",
    "        'Text': lambda x: ' | '.join(x), \n",
    "    })\n",
    ")\n",
    "\n",
    "rule['summary'] = rule['Text'].apply(generate_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a mantener este dataset que contiene los resúmenes de las tres páginas y los almacenaremos en un dataframe\n",
    "# We will keep this dataset containing the summaries of the three pages and store it in a dataframe\n",
    "\n",
    "rule_compendium = rule[['section','questions','rule', 'decision', 'link', 'authority', 'date', 'case_file', 'summary']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haremos expresa la metadata de las decisiones para que le sirva al modelo de lenguaje posteriormente\n",
    "# We will make the metadata of the decisions explicit so it can be used by the language model later\n",
    "\n",
    "rule['summary'] = rule.apply(lambda row: f'Resumen:{row[\"summary\"]} El resumen pertenece a la Resolución {row[\"decision\"]} emitida por la Autoridad:{row[\"authority\"]}', axis=1)\n",
    "\n",
    "rule = (\n",
    "    rule\n",
    "    .groupby(['section','questions','rule'], as_index=False)\n",
    "    .agg({\n",
    "        'summary': lambda x: ' | '.join(x),  # Combinar todos los resúmenes y tenerlos listos para responder a las preguntas\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a generar el input para nuestro modelo de lenguaje\n",
    "# We will generate the input for our language model\n",
    "\n",
    "\n",
    "rule['input'] = rule.apply(lambda row: f'Jurisprudencia Relevante:\\n{row[\"summary\"]}\\nPreguntas a Responder:\\n{row[\"questions\"]}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función va a responder a las preguntas específicas utilizando nuestra base de conocimiento de jurisprudencia.\n",
    "# This function will answer specific questions using our jurisprudence knowledge base.\n",
    "\n",
    "def answering_issue_per_rule(text, max_tokens=2013, temperature=0, top_p=1):\n",
    "    try:\n",
    "        message = anthropic.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            system=\n",
    "                \"Responde a las Preguntas planteadas a partir de la Jurisprudencia Relevante. Cita tus respuestas con (p.XX, Resolución XXX).\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }]\n",
    "        )\n",
    "        return message.content[0].text if isinstance(message.content, list) else message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "rule['answering_issue_per_rule'] = rule['input'].apply(answering_issue_per_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenamos la información intermedia que sirve como fuente de verdad para la Inteligencia Artificial\n",
    "# We store the intermediate information that serves as the truth source for the Artificial Intelligence\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "doc.add_heading(\"Jurisprudencia Relevante\", level=1)\n",
    "for _, row in rule.iterrows():\n",
    "    text = (\n",
    "        f\"Las siguientes preguntas: '{row['questions']}' sirven para interpretar el siguiente artículo: '{row['rule']}'\\n\\n\"\n",
    "        f\"El siguiente texto contiene las respuestas a las preguntas: '{row['answering_issue_per_rule']}'\"\n",
    "        f\"Los resumenes utilizados son los siguientes:\\n\\n{row['summary']}\\n\\n\"\n",
    "    )\n",
    "    doc.add_paragraph(text)\n",
    "output_path = (folder_path + '\\\\jurisprudencia_relevante.docx')\n",
    "doc.save(output_path)\n",
    "\n",
    "rule_compendium.to_excel(folder_path + '\\\\metadata_jurisprudencia.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Módulo de Aplicación / Application Module\n",
    "\n",
    "Este módulo se centra en utilizar las respuestas a las preguntas específicas para interpretar un artículo de la ley de competencia en relación con los hechos del caso. El proceso incluye varios pasos estructurados para garantizar un análisis lógico y coherente:\n",
    "\n",
    "1. **Interpretación Específica por Artículo**  \n",
    "   Se emplea un modelo de lenguaje, contenido en la función `def application()`, instruido específicamente para razonar de manera lógica y coherente. Este modelo evalúa aspectos clave sobre la aplicación de un artículo específico de la ley a los hechos del caso.\n",
    "\n",
    "2. **Agrupación de Respuestas por Área de Enfoque**  \n",
    "   Después de obtener las respuestas para cada artículo aplicable, estas se agrupan en dos categorías:  \n",
    "   - **Ámbito de Aplicación**: Respuestas que abordan la aplicabilidad de las normas referidas al ámbito de aplicación.  \n",
    "   - **Interpretación de Prácticas Colusorias Horizontales**: Respuestas que analizan específicamente la aplicación de los artículos vinculados a las prácticas colusorias horizontales.  \n",
    "\n",
    "   Luego, la misma función de aplicación se aplica a todos los informes relacionados.\n",
    "\n",
    "3. **Consolidación de Respuestas**  \n",
    "   Los dos grupos de respuestas se combinan en un objeto denominado `ira_text`. Este objeto contiene un resumen condensado pero completo sobre la aplicación de cada artículo a los hechos del caso. La consolidación garantiza que la información sea esencial y precisa, proporcionando una base sólida para responder al **issue**.\n",
    "\n",
    "Al estructurar sistemáticamente la aplicación de cada artículo, este módulo ofrece un análisis claro, lógico y orientado al contexto para evaluar el caso bajo la ley de competencia.\n",
    "\n",
    "---\n",
    "\n",
    "This module focuses on utilizing the answers to specific questions to interpret an article of competition law in relation to the facts of the case. The process includes several structured steps to ensure logical and coherent analysis:\n",
    "\n",
    "1. **Article-Specific Interpretation**  \n",
    "   A language model, contained in the function `def application()`, is employed, specifically instructed to reason logically and coherently. This model evaluates key aspects regarding the application of a specific article of the law to the facts of the case.\n",
    "\n",
    "2. **Grouping Responses by Focus Area**  \n",
    "   After obtaining the answers for each applicable article, these are grouped into two categories:  \n",
    "   - **Scope of Application**: Responses addressing the applicability of rules related to the scope of application.  \n",
    "   - **Interpretation of Horizontal Collusive Practices**: Responses specifically analyzing the application of articles linked to horizontal collusive practices.  \n",
    "\n",
    "   Then, the same application function is applied to all related reports.\n",
    "\n",
    "3. **Consolidating Responses**  \n",
    "   The two sets of responses are combined into an object called `ira_text`. This object contains a condensed yet complete summary of the application of each article to the facts of the case. The consolidation ensures that the information is essential and precise, providing a solid foundation to address the **issue**.\n",
    "\n",
    "By systematically structuring the application of each article, this module delivers a clear, logical, and context-driven analysis to evaluate the case under competition law.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a generar el input para nuestro modelo de lenguaje\n",
    "# We will generate the input for our language model\n",
    "\n",
    "rule['input_2'] = rule.apply(lambda row: f'Artículo a Aplicar:{row[\"rule\"]}\\nAyuda:\\n{row[\"answering_issue_per_rule\"]}\\nHechos del Caso:\\n{query}', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application(text, max_tokens=2013, temperature=0, top_p=1):\n",
    "    \"\"\"\n",
    "    Función para generar una respuesta utilizando un modelo de lenguaje a gran escala.\n",
    "    Este modelo analiza un texto proporcionado en base a un conjunto de instrucciones predefinidas \n",
    "    relacionadas con el análisis de un artículo legal.\n",
    "\n",
    "    Parámetros:\n",
    "        text (str): El texto de entrada para ser analizado.\n",
    "        max_tokens (int): Número máximo de tokens que puede generar el modelo en la respuesta. \n",
    "                          Valor por defecto: 2013.\n",
    "        temperature (float): Parámetro que controla la aleatoriedad de la respuesta. \n",
    "                             Valores bajos como 0 producen resultados más deterministas. Valor por defecto: 0.\n",
    "        top_p (float): Parámetro que controla la probabilidad acumulada para filtrar las respuestas generadas. \n",
    "                       Valor por defecto: 1 (sin filtro).\n",
    "\n",
    "    Retorna:\n",
    "        str: El contenido generado por el modelo, dependiendo del análisis realizado.\n",
    "        None: En caso de ocurrir una excepción, se devuelve `None` y se imprime el error.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        message = anthropic.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            system=\n",
    "                \"Vamos a pensar paso.\"\n",
    "                \"1. ÁMBITO DE APLICACIÓN DEL ARTÍCULO:\"\n",
    "                \"- Identifica primero a qué situaciones/casos aplica el artículo\"\n",
    "                \"- Determina qué situaciones/casos están excluidos\"\n",
    "                \"- Si el caso no está dentro del ámbito, detén el análisis aquí\"\n",
    "                \"2. Solo si el caso está dentro del ámbito:\"\n",
    "                \"- Extrae los demás criterios/requisitos específicos del Artículo\"\n",
    "                \"- Identifica qué elementos deben probarse según el Artículo\"\n",
    "                \"3. ANÁLISIS DE HECHOS:\"\n",
    "                \"- Verifica si los hechos están dentro del ámbito de aplicación\"\n",
    "                \"- Solo si están en el ámbito, analiza si cumplen los demás criterios\"\n",
    "                \"4. CONCLUSIÓN:\"\n",
    "                \"- Si los hechos no están en el ámbito, concluye que no aplica\"\n",
    "                \"- Solo si están en el ámbito, analiza y concluye sobre los demás elementos\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }]\n",
    "        )\n",
    "        return message.content[0].text if isinstance(message.content, list) else message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule['application_1'] = rule['input_2'].apply(application)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira = (\n",
    "    rule\n",
    "    .groupby(['section'], as_index=False)\n",
    "    .agg({\n",
    "        # Combinamos todas las respuestas por sección de análisis (ámbito de aplicación y acreditación de prácticas colusorias)\n",
    "        # We combine all the answers by analysis section (scope of application and accreditation of collusive practices)\n",
    "        \n",
    "        'application_1': lambda x: ' | '.join(x), \n",
    "    })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira['application_1'] = ira.apply(lambda row: f'Opiniones sobre los Artículos:{row[\"application_1\"]}', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira_aa = ira[ira['section'] == 'aa']\n",
    "ira_pc = ira[ira['section'] == 'pc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def practicas_colusorias(text, max_tokens=2013, temperature=0, top_p=1):\n",
    "    try:\n",
    "        message = anthropic.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            system=\n",
    "                \"Vamos a pensar paso.\"\n",
    "                \"Determina si es una práctica colusoria a la que debería aplicarse la prohibición absoluta o relativa.\"\n",
    "                \"SI es inter marca Y accesoria a un acuerdo lícito Y tiene como objetivo explícito la fijación de precios o condiciones comerciales que inciden directamente en el precio, directamente en reparto de mercado,directamente en limitación de producción o reparto de contrataciones con el Estado, entonces es una prohibición absoluta.\"\n",
    "                \"SI uno de los requisitos antes mencionados no se satisface, entonces aplica la prohibición relativa y el balance de efectos correspondiente.\"\n",
    "                \"Debes dar una opinión sobre la existencia de una conducta anticompetitiva.\"\n",
    "                \"Responde en el siguiente formato:\"\n",
    "                \"Análisis: (contraste de argumentos a favor de aplicar prohibición absoluta y argumentos a favor de aplicar prohibición relativa. Las prácticas sujetas a la prohibición absoluta no pueden ser exclusorias, es decir, si afecta la permanencia de competidores en el mercado y crea barreras a la entrada, es una prohibición relativa\"\n",
    "                \"Respuesta: (no adelantar la respuesta hasta desarrollar el análisis)\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }]\n",
    "        )\n",
    "        return message.content[0].text if isinstance(message.content, list) else message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_2800\\1527804548.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ira_aa['application_2'] = ira_aa['application_1'].apply(application)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_2800\\1527804548.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ira_pc['application_2'] = ira_pc['application_1'].apply(practicas_colusorias)\n"
     ]
    }
   ],
   "source": [
    "ira_aa['application_2'] = ira_aa['application_1'].apply(application)\n",
    "ira_pc['application_2'] = ira_pc['application_1'].apply(practicas_colusorias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira =pd.concat([ira_aa,ira_pc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira_text = \" \".join(ira['application_2'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ira_resumen(text, max_tokens=2013, temperature=1, top_p=1):\n",
    "    try:\n",
    "        message = anthropic.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            system=\n",
    "                \"Sintetiza las conclusiones de la siguiente información\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }]\n",
    "        )\n",
    "        return message.content[0].text if isinstance(message.content, list) else message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira_text = ira_resumen(ira_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ira_text = f'¨Reglas-Aplicación: {ira_text} {issue}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos los documentos revisados por la Inteligencia Artificial para posterior revisión\n",
    "# We generate the documents reviewed by the Artificial Intelligence for later review\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "doc.add_heading(\"Reglas y Aplicación\", level=1)\n",
    "\n",
    "for _, row in ira.iterrows():\n",
    "    text = (\n",
    "        f\"Los siguientes textos pertenecen a la sección '{row['section']}'\\n\\n\"\n",
    "        f\"El siguiente texto contiene la síntesis de las respuestas sobre la aplicación de los artículos:\\n'{row['application_2']}'\\n\\n\"\n",
    "        f\"Los siguientes textos contienen el análisis referido a la aplicación de un artículo concreto a los hechos del caso:\\n\\n{row['application_1']}\\n\\n\"\n",
    "    )\n",
    "    doc.add_paragraph(text)\n",
    "output_path = (folder_path + '\\\\reglas_aplicación.docx')\n",
    "doc.save(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Módulo de Conclusión / Conclusion Module\n",
    "\n",
    "En este módulo, se utiliza una función que emplea un modelo de lenguaje con un prompt de Chain of Thought (CoT). La función recibe un texto como entrada y analiza si se cumplen los requisitos legales relacionados con el ámbito de aplicación (objetivo, subjetivo y territorial). Además, identifica si la práctica en cuestión es una prohibición absoluta o relativa, y si es relativa, evalúa si los efectos negativos son significativos y superan a los positivos, lo que justificaría que la práctica sea considerada prohibida.\n",
    "\n",
    "La función devuelve un análisis detallado y una conclusión clara sobre si los hechos del caso constituyen una vulneración de la ley de competencia, considerando todos los aspectos relevantes.\n",
    "\n",
    "---\n",
    "\n",
    "In this module, a function is used that employs a language model with a Chain of Thought (CoT) prompt. The function takes a text input and analyzes whether the legal requirements related to the scope of application (objective, subjective, and territorial) are met. Additionally, it identifies whether the practice in question is an absolute or relative prohibition, and if it is relative, it assesses whether the negative effects are significant and outweigh the positive ones, which would justify considering the practice as prohibited.\n",
    "\n",
    "The function returns a detailed analysis and a clear conclusion on whether the facts of the case constitute a violation of competition law, considering all relevant aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion(text, max_tokens=2013, temperature=1, top_p=1):\n",
    "    try:\n",
    "        message = anthropic.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            system=\n",
    "                \"Vamos a pensar paso.\"\n",
    "                \"Vas a responder a la pregunta planteada a partir de la información proporcionada.\" \n",
    "                \"Debes precisar si el ámbito de aplicación objetivo, subjetivo y territorial se cumplen. En caso no se cumplan, no se puede sancionar\"\n",
    "                \"Debes identificar si se trata de una prohibición absoluta o relativa.\"\n",
    "                \"Si se trata de una prohibición relativa, debes analizar si los efectos negativos existen, son significativos y superan a los efectos positivos para que sea una práctica prohibida\"\n",
    "                \"Responde en el siguiente formato\"\n",
    "                \"Análisis\"\n",
    "                \"Respuesta: (no debes adelantar tu respuesta, la respuesta debe ir como conclusión a tu análisis)\",\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }]\n",
    "        )\n",
    "        return message.content[0].text if isinstance(message.content, list) else message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "irac = conclusion(ira_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw API response content: ```json\n",
      "{\n",
      "  \"anticompetitive\": false,\n",
      "  \"grounds\": [\n",
      "    \"No cumple con los requisitos para ser sancionada porque, aunque es una práctica colusoria horizontal intramarca sujeta a prohibición relativa, no se demuestran efectos negativos significativos en el mercado.\",\n",
      "    \"La duración de los efectos fue corta, solo 3 meses, y no afectó el volumen de servicios.\",\n",
      "    \"Los precios acordados se mantuvieron dentro del rango de mercado, y existían abundantes alternativas competitivas, ya que los talleres multimarca representaban el 60% del mercado.\",\n",
      "    \"No se evidencian efectos negativos significativos que superen los beneficios.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Extracted Valid Data\n"
     ]
    }
   ],
   "source": [
    "def conclusion_final(query, model):\n",
    "    if not isinstance(query, str):\n",
    "        raise ValueError(\"Input must be a string.\")\n",
    "\n",
    "    # Prompt\n",
    "    prompt = (\n",
    "        \"Extrae y organiza los datos del esquema JSON sobre la investigación anticompetitiva. \"\n",
    "        \"En 'anticompetitive', indica con un valor booleano (true o false) si hubo una vulneración a la ley de competencia. \"\n",
    "        \"En 'grounds', proporciona una explicación detallada de las razones por las cuales hubo o no una vulneración a la ley de competencia. \"\n",
    "        \"Devuelve exclusivamente en formato JSON, sin explicaciones ni comentarios, con el siguiente esquema:\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"anticompetitive\\\": true o false,\\n\"\n",
    "        \"  \\\"grounds\\\": [\\n\"\n",
    "        \"    \\\"Explicación detallada de las razones\\\"\\n\"\n",
    "        \"  ]\\n\"\n",
    "        \"}\"\n",
    "    )\n",
    "    # API request\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt + \"\\n\\n\" + query}],\n",
    "        temperature=0.95,\n",
    "        max_tokens=1500,\n",
    "    )\n",
    "\n",
    "    # Extract JSON from response\n",
    "    response_content = response.choices[0].message['content']\n",
    "    print(\"Raw API response content:\", response_content)  # Debugging\n",
    "\n",
    "    # Improved JSON extraction\n",
    "    \n",
    "    json_content = extract_json_from_response(response_content)\n",
    "    if not json_content:\n",
    "        print(\"No valid JSON found in response.\")\n",
    "        return None\n",
    "\n",
    "    # Parse and validate the JSON\n",
    "    try:\n",
    "        data = json.loads(json_content)  # Parse JSON\n",
    "        validate(instance=data, schema=schema['schema'])  # Validate with updated schema\n",
    "        return data\n",
    "    except ValidationError as ve:\n",
    "        print(\"Validation error:\", ve.message)\n",
    "        print(\"Offending data:\", json_content)\n",
    "        return None\n",
    "    except json.JSONDecodeError as je:\n",
    "        print(\"JSON parsing error:\", je)\n",
    "        print(\"Offending response content:\", response_content)\n",
    "        return None\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    model = \"gpt-4o\"  # Change this to the desired model\n",
    "\n",
    "    # Use the joined_text as the query input\n",
    "    try:\n",
    "        result = conclusion_final(irac, model)\n",
    "        if result:\n",
    "            print(\"Extracted Valid Data\")\n",
    "        else:\n",
    "            print(\"No valid data extracted.\")\n",
    "    except ValueError as ve:\n",
    "        print(\"Input error:\", ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_final = pd.DataFrame(result)\n",
    "prediction_final = (\n",
    "    prediction_final\n",
    "    .groupby(['anticompetitive'], as_index=False)\n",
    "    .agg({\n",
    "        'grounds': lambda x: ''.join(x),\n",
    "    })\n",
    ")\n",
    "prediction_final.to_excel(folder_path + '\\\\predicción_ljp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera el Informe IRAC\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "doc.add_heading(\"Informe IRAC\", level=1)\n",
    "\n",
    "text = (\n",
    "    f\"Issue:{issue}\\n\"\n",
    "    f\"RAC:\\n'{irac}'\\n\"\n",
    ")\n",
    "doc.add_paragraph(text)\n",
    "\n",
    "output_path = (folder_path + '\\\\irac.docx')\n",
    "doc.save(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
